{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing \n",
    "\n",
    "This jupyter notebook deals with data preprocessing for further analysis in the PyClone, SciClone and TitanCNA libraries. A prepared .vcf file and data obtained from the variant calling process are required for launch. For the preparation of variants, it is recommended to use the CNVpytor library, which ensures that the resulting data is immediately suitable for preprocessing without further modifications. When using other libraries, it is necessary to edit the data into the following format.\n",
    "\n",
    "\n",
    "\n",
    "## Required libraries\n",
    "\n",
    "Libraries necessary for the smooth running of the program (Dependencies excluded):\n",
    "\n",
    "- [io](https://docs.python.org/3/library/io.html) - Core tools for working with streams\n",
    "- [os](https://docs.python.org/3/library/os.html) - Miscellaneous operating system interfaces\n",
    "- [pandas](https://pandas.pydata.org)  - Provides fast, flexible, and expressive data structures \n",
    "- [numpy](https://numpy.org) - Library used for working with arrays.\n",
    "- [PyVCF](https://pyvcf.readthedocs.io/en/latest/) - A Variant Call Format Parser for Python\n",
    "- [json](https://docs.python.org/3/library/json.html) - JSON encoder and decoder\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html) - Object-oriented filesystem paths\n",
    "- [csv](https://docs.python.org/3/library/csv.html) - CSV File Reading and Writing\n",
    "- [typing](https://docs.python.org/3/library/typing.html) - Support for type hints\n",
    "\n",
    "For managing dependencies, we recommend using a package management system such as [Conda](https://docs.conda.io/en/latest/).\n",
    "\n",
    "\n",
    "\n",
    "## CNVpytor / CNVnator\n",
    "\n",
    "Both libraries are suitable for preparing variants as they work identically and return the same formats. CNVpytor serves as an extension of the CNVnator library for compatibility with the Python programming language.\n",
    "\n",
    "[CNVpytor ](https://github.com/abyzovlab/CNVpytor)- Official documentation\n",
    "\n",
    "[CNVpytor - Installer](https://anaconda.org/bioconda/cnvpytor) - Installation through conda\n",
    "\n",
    "[CNVnator](https://github.com/abyzovlab/CNVnator) - Official documentation\n",
    "\n",
    "\n",
    "\n",
    "### Format of variant calling results file (.tsv)\n",
    "\n",
    "If you decide to use another library for the preparation of variants, you must follow the following guidelines. In general, this file must be in .tsv (Tab separated values) format. Values can be separated by tabs or number of whitespaces. The program does not recognizes other types of separators and will ignore them. **Columns required:**\n",
    "\n",
    "- CNV type - \"deletion\" or \"duplication\"\n",
    "- CNV region - chr:start-end\n",
    "- CNV size - end-start\n",
    "- CNV level - read depth normalized to 1\n",
    "- e-val1 - e-value (p-value multiplied by genome size divided by bin size) calculated using t-test statistics between RD statistics in the region and global\n",
    "- e-val2 - e-value (p-value multiplied by genome size divided by bin size) from the probability of RD values within the region to be in the tails of a gaussian distribution of binned RD\n",
    "- e-val3 - same as e-val1 but for the middle of CNV\n",
    "- e-val4 - same as e-val2 but for the middle of CNV\n",
    "- q0 - fraction of reads mapped with q0 quality in call region\n",
    "- pN - fraction of reference genome gaps (Ns) in call region\n",
    "- dG - distance from closest large (>100bp) gap in reference genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell dealing with imports \n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import json\n",
    "from pathlib import Path\n",
    "import mutation as mutation\n",
    "from mutation import Mutation\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell defining used custom functions\n",
    "\n",
    "# Function that reads .vcf file and transforms it into dataframe.\n",
    "def load_vcf_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [l for l in f if not l.startswith('##')]\n",
    "    return pd.read_csv(\n",
    "        io.StringIO(u''.join(lines)),\n",
    "        dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n",
    "               'QUAL': str, 'FILTER': str, 'INFO': str},\n",
    "        sep='\\t'\n",
    "    ).rename(columns={'#CHROM': 'CHROM'})\n",
    "\n",
    "\n",
    "\n",
    "def write_headers_as_json(vcf_reader, fn_out):\n",
    "    \"\"\"\n",
    "    Once the vcf if loaded, put all the metadata from the header into\n",
    "    a single json object and pretty print it to a file.\n",
    "    Has no effect on the Reader but creates a new file\n",
    "    \"\"\"\n",
    "    header_json = dict()\n",
    "\n",
    "    header_json[\"metadata\"] = vcf_reader.metadata\n",
    "    header_json[\"infos\"] =   vcf_reader.infos\n",
    "    header_json[\"filters\"] = vcf_reader.filters\n",
    "    header_json[\"formats\"] = vcf_reader.formats\n",
    "    header_json[\"samples\"] = vcf_reader.samples\n",
    "\n",
    "    print(\"writing header info as json to : \" + str(fn_out))\n",
    "    with open(fn_out, 'w') as outfile:\n",
    "        json.dump(header_json, outfile, indent=4)\n",
    "    return\n",
    "\n",
    "def extract_sample_id(input_filename):\n",
    "    \"\"\"\n",
    "    infers a sample_id from the input filename of the vcf file.\n",
    "    The result is the basename without the suffix (.vcf)\n",
    "    \"\"\"\n",
    "    p = Path(input_filename)\n",
    "    sample_id = (p.stem)\n",
    "    return sample_id\n",
    "\n",
    "def _construct_mutation_id(vcf_record):\n",
    "        \"\"\"\n",
    "        The id of the mutation of the record has format\n",
    "            chrom:pos:ref\n",
    "        \"\"\"\n",
    "        chrom = str(vcf_record.CHROM)\n",
    "        pos = str(vcf_record.POS)\n",
    "        ref = str(vcf_record.REF)\n",
    "        #mid = (chrom + \":\" + pos + \":\" + ref).decode('utf-8')\n",
    "        mid = (chrom + \":\" + pos)\n",
    "        return mid\n",
    "\n",
    "\n",
    "\n",
    "def write_pyclone_inputs(out_dirname, mutations):\n",
    "    \"\"\"\n",
    "    write the mutations to a set of tsv files, one per sample, as described\n",
    "    by the pyclone docs: https://github.com/Roth-Lab/pyclone\n",
    "    \"\"\"\n",
    "    print(\"writing mutations as pyclone input format to: \" + str(out_dirname))\n",
    "\n",
    "    fieldnames = ['mutation_id', 'ref_counts', 'var_counts',\n",
    "        'major_cn', 'minor_cn', 'normal_cn']\n",
    "\n",
    "    #index the mutations by sample_id\n",
    "    mutations_map = dict()\n",
    "    for m in mutations:\n",
    "        sample_id = m.sample_id\n",
    "        if not sample_id in mutations_map:\n",
    "            mutations_map[sample_id] = list()\n",
    "        mutations_map[sample_id].append(m)\n",
    "\n",
    "    for sample_id in mutations_map:\n",
    "        out_fn = Path(out_dirname, (sample_id + '.tsv'))\n",
    "\n",
    "        with open(str(out_fn), 'w') as csvfile:\n",
    "\n",
    "            writer = csv.DictWriter(csvfile,\n",
    "                fieldnames=fieldnames,\n",
    "                delimiter='\\t',\n",
    "                quotechar='\"',\n",
    "                quoting=csv.QUOTE_MINIMAL,\n",
    "                extrasaction='ignore')\n",
    "\n",
    "            writer.writeheader()\n",
    "            for m in mutations_map[sample_id]:\n",
    "                md  = vars(m)\n",
    "                #pyclone calls them 'var' not 'alt'\n",
    "                md['var_counts'] = md['alt_counts']\n",
    "                writer.writerow(md)\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the file, argument number 1, which was filled in at startup, is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and making dataframe out of .vcf file\n",
    "main_vcf = load_vcf_file('./DO52567.vcf')\n",
    "main_vcf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next preprocessing step consists of filtering the loaded .vcf file. The file consists of a header and records. We can ignore the header as it is not necessary for our further analysis. For the subsequent search and analysis of the records, we select only the following columns from the main .vcf file: ID, POS, CHROM, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1655518d54ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Creating empty dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiltered_main_vcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Copying needed columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Filtering columns from main .vcf file.\n",
    "\n",
    "# Creating empty dataframe\n",
    "filtered_main_vcf = pd.DataFrame()\n",
    "\n",
    "# Copying needed columns\n",
    "filtered_main_vcf['POS'] = main_vcf['POS'].values\n",
    "filtered_main_vcf['CHR'] = main_vcf['CHROM'].values\n",
    "\n",
    "# Extracting needed information by splitting columns\n",
    "filtered_main_vcf[['GENOTYPE', 'ALLELIC DEPTH', 'DEPTH', 'GENOTYPE QUALITY', 'PHRED-SCALED LIKELIHOODS']] = main_vcf.iloc[:, 9].str.split(\":\",expand=True,)\n",
    "\n",
    "filtered_main_vcf.drop(['GENOTYPE QUALITY', 'PHRED-SCALED LIKELIHOODS'], axis=1)\n",
    "filtered_main_vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting needed information by splitting columns\n",
    "temp = main_vcf['INFO'].str.split(\";\",expand=True)\n",
    "temp\n",
    "\n",
    "# Extracting Minor and Major allele copy number \n",
    "filtered_main_vcf['ALLELIC FREQUENCY'] = temp[1].str[3:]\n",
    "filtered_main_vcf['MINOR ALLELE COPY NUMBER'] = np.where(filtered_main_vcf['GENOTYPE'] == '0/1', 1, 0)\n",
    "filtered_main_vcf['MAJOR ALLELE COPY NUMBER'] = np.where(filtered_main_vcf['GENOTYPE'] == '1/1', 2, 1)\n",
    "\n",
    "\n",
    "chr1_vcf = filtered_main_vcf[filtered_main_vcf[\"CHR\"] == 'chr1']\n",
    "chr1_vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnvnator_df = pd.read_csv('test.tsv',  names=['CNV Type','CNV Region','CNV size','CNV level','e-val1', 'e-val2', 'e-val3','e-val4','q0','pN','dG'], delimiter=r\"\\s+\")\n",
    "\n",
    "cnvnator_df\n",
    "\n",
    "ranges = cnvnator_df['CNV Region'].str.split(':',expand=True)\n",
    "new = pd.DataFrame()\n",
    "new[['start','end']] = ranges[1].str.split('-',expand=True)\n",
    "new['COPY NUMBER'] = cnvnator_df['CNV level']\n",
    "ranges = ranges[ranges[0] == '1']\n",
    "new['start'] = new['start'].astype('int')\n",
    "new['end'] = new['end'].astype('int')\n",
    "\n",
    "new = new[new['start'] != 1]\n",
    "chr1_vcf['POS'] = chr1_vcf['POS'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr1_vcf = chr1_vcf.assign(key = 1)\n",
    "new = new.assign(key = 1)\n",
    "merged = chr1_vcf.merge(new[['key', 'start', 'end', 'COPY NUMBER']], on='key')\n",
    "filtered = merged[(merged['POS'] >= merged['start']) & (merged['POS'] <= merged['end'])]\n",
    "filtered['COPY NUMBER'] = filtered['COPY NUMBER'].astype('float')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['COPY NUMBER'] = filtered['COPY NUMBER'].apply(np.round)\n",
    "filtered.sort_values(by=['COPY NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (filtered['POS'].isin(chr1_vcf[\"POS\"]).any(0)):\n",
    "    chr1_vcf['COPY NUMBER'] = filtered['Copy Number']\n",
    "else:\n",
    "    chr1_vcf['COPY NUMBER'] = 2\n",
    "\n",
    "chr1_vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Left join on 'position'\n",
    "merged_df = chr1_vcf.merge(filtered, on='POS', how='left')\n",
    "merged_df['COPY NUMBER'] = merged_df['COPY NUMBER'].fillna(2)\n",
    "merged_df[['POS', 'MINOR ALLELE COPY NUMBER_x', 'MAJOR ALLELE COPY NUMBER_x', 'COPY NUMBER']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "sciclone_df = pd.DataFrame()\n",
    "\n",
    "final_df['mutation_id'] = 'chr1:' + merged_df['POS'].astype(str)\n",
    "\n",
    "sciclone_df['chr'] = 'chr1'\n",
    "sciclone_df['pos'] = merged_df['POS'].values\n",
    "\n",
    "final_df[['ref_counts', 'var_counts', 'none']] = merged_df['ALLELIC DEPTH_x'].str.split(',',expand=True)\n",
    "sciclone_df['ref_reads'] = final_df['ref_counts'].values\n",
    "sciclone_df['var_reads'] = final_df['var_counts'].values\n",
    "sciclone_df['vaf'] = final_df['var_counts'].astype(int) / (final_df['ref_counts'].astype(int) + final_df['var_counts'].astype(int))\n",
    "final_df['minor_cn'] = merged_df['MINOR ALLELE COPY NUMBER_x'].values\n",
    "final_df['major_cn'] = merged_df['MAJOR ALLELE COPY NUMBER_x'].values\n",
    "final_df['normal_cn'] = merged_df['COPY NUMBER'].values\n",
    "final_df['normal_cn'] = final_df['normal_cn'].astype('int')\n",
    "final_df = final_df.drop('none', axis=1)\n",
    "sciclone_df['chr'] = 'chr1'\n",
    "sciclone_df.to_csv('sciclone2.tsv', sep=\"\\t\")\n",
    "sciclone_df\n",
    "sciclone_exclude = pd.DataFrame(columns=['chromosomes', 'start', 'end'])\n",
    "sciclone_exclude['chromosomes'] = 'chr1'\n",
    "sciclone_exclude['start'] = 1\n",
    "sciclone_exclude['end'] = 5\n",
    "\n",
    "df2 = {'chromosomes': 'chr1', 'start': 1, 'end': 5}\n",
    "sciclone_exclude = sciclone_exclude.append(df2, ignore_index = True)\n",
    "\n",
    "sciclone_exclude.to_csv('sciclone_exclude.tsv', sep=\"\\t\")\n",
    "\n",
    "sciclone_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciclone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciclone_df2 = pd.DataFrame()\n",
    "sciclone_dffinal = pd.DataFrame()\n",
    "\n",
    "sciclone_df2[['start','end']] = ranges[1].str.split('-',expand=True)\n",
    "sciclone_df2['start'] = sciclone_df2['start'].astype('int')\n",
    "sciclone_df2['end'] = sciclone_df2['end'].astype('int')\n",
    "sciclone_df2['segment_mean'] = cnvnator_df['CNV level']\n",
    "sciclone_df2['segment_mean'] = sciclone_df2['segment_mean'].apply(np.round)\n",
    "sciclone_df2['chr'] = \"chr1\"\n",
    "sciclone_df2\n",
    "sciclone_dffinal['chr'] = sciclone_df2['chr'].values\n",
    "sciclone_dffinal['start'] = sciclone_df2['start'].values\n",
    "sciclone_dffinal['end'] = sciclone_df2['end'].values\n",
    "sciclone_dffinal['segment_mean'] = sciclone_df2['segment_mean'].values\n",
    "\n",
    "sciclone_dffinal.to_csv('sciclone.tsv', sep=\"\\t\")\n",
    "sciclone_dffinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('example.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PyClone run_analysis_pipeline --in_files example_subset.tsv --working_dir ./finalTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_reader = vcf.Reader\n",
    "vcf_reader = vcf.Reader(open(\"DO52567.vcf\", 'r'))\n",
    "\n",
    "write_headers_as_json(vcf_reader, 'headers.json')\n",
    "\n",
    "sample_id = extract_sample_id(\"DO52567.vcf\")\n",
    "\n",
    "\n",
    "mutation_list = []\n",
    "#print(vcf_reader.metadata)\n",
    "tumor_samples = vcf_reader.metadata[\"tumor_sample\"]\n",
    "for rec in vcf_reader:\n",
    "    for sample in tumor_samples:\n",
    "        mutation_id = _construct_mutation_id(rec)\n",
    "        tumor_call = rec.genotype(sample)\n",
    "        call_data = tumor_call.data\n",
    "        print(call_data)\n",
    "        counts = call_data.AD\n",
    "        ref_counts = counts[0]\n",
    "        alt_counts = counts[1]\n",
    "        mutation_list.append(Mutation(sample, mutation_id, ref_counts, alt_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['POS'] == 248897833].minor_cn\n",
    "minor_cn = final_df[final_df['POS'] == rec.POS].minor_cn\n",
    "major_cn = final_df[final_df['POS'] == rec.POS].major_cn\n",
    "normal_cn = final_df[final_df['POS'] == rec.POS].normal_cn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating input data for PyClone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "write_pyclone_inputs(\"output\", mutation_list)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating input data for SciClone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating input data for TitanCNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_tsv = pd.read_csv('output/DO52567.tsv', sep='\\t')\n",
    "subset_tsv = subset_tsv.loc[0:200]\n",
    "subset_tsv\n",
    "subset_tsv.to_csv('example_subset.tsv', sep=\"\\t\")\n",
    "subset_tsv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciClone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sciclone = pd.read_csv('sciclone2.tsv', sep='\\t')\n",
    "subset_sciclone.drop(columns=subset_sciclone.columns[0], axis=1, inplace=True)\n",
    "subset_sciclone = subset_sciclone.loc[0:200]\n",
    "subset_sciclone.to_csv('sciclone_subset.tsv', sep=\"\\t\")\n",
    "subset_sciclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = chr1_vcf['POS'].isin(range(new['start'], new['end'] + 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TitanCNA\n",
    "\n",
    "To prepare the necessary files for the TitanCNA library, we need to prepare two specific file types - .het (heterozygosity file) and .cn (copy number  file). Both of these files are required by the TitanCNA library in following formats (Observing the order and nomenclature of the columns is recommended):\n",
    "\n",
    "#### **Heterozygosity file**\n",
    "\n",
    "- **chr -** Chromosome name\n",
    "- **posn -** Mutation position\n",
    "- **ref -** Refference allele\n",
    "- **refCount -** Refference allele count\n",
    "- **Nref -** Alternative Allele\n",
    "- **NrefCount -** Alternative Allele count\n",
    "\n",
    "\n",
    "\n",
    "#### Copy number file\n",
    "\n",
    "- **chr - **Chromosome name\n",
    "- **start -** Start of the segment\n",
    "- **end -** End of the segment\n",
    "- **logR -** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of .het file\n",
    "titnacna_df = pd.DataFrame()\n",
    "\n",
    "titnacna_df['chr'] = sciclone_df['chr'].values\n",
    "titnacna_df['posn'] = sciclone_df['pos'].values\n",
    "titnacna_df['ref'] = main_vcf['REF']\n",
    "titnacna_df['refCount'] = sciclone_df['ref_reads'].values\n",
    "titnacna_df['Nref'] = main_vcf['ALT']\n",
    "titnacna_df['NrefCount'] = sciclone_df['var_reads'].values\n",
    "titnacna_df['chr'] = 1\n",
    "titnacna_df.to_csv('input_files/TitanCNA/titancna.het', sep=\"\\t\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64a3e5206a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creation of .cn file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtitnacna_df2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtitnacna_df2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msciclone_df2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtitnacna_df2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtitnacna_df2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msciclone_df2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Creation of .cn file\n",
    "titnacna_df2 = pd.DataFrame()\n",
    "titnacna_df2['chr'] = sciclone_df2['chr'].values\n",
    "titnacna_df2['chr'] = 1\n",
    "titnacna_df2['start'] = sciclone_df2['start'].values\n",
    "titnacna_df2['end'] = sciclone_df2['end'].values\n",
    "titnacna_df2['logR'] =  sciclone_df2['segment_mean'].values\n",
    "\n",
    "titnacna_df2.to_csv('input_files/TitanCNA/titancna.cn', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pd.DataFrame()\n",
    "for index, row in chr1_vcf.iterrows():\n",
    "    for index2, row2 in new.iterrows():\n",
    "        if ((row['POS'] >= row2['start']) & (row['POS'] <= row2['end'])):\n",
    "            mask.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.assign(key = 1)\n",
    "df2 = df2.assign(key = 1)\n",
    "merged = df1.merge(df2[['key', 'start', 'end']], on='key')\n",
    "filtered = merged[(merged['position'] >= merged['start']) & (merged['position'] <= merged['end'])]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (chr1_vcf['POS'] >= new['start'])\n",
    "mask2 = (chr1_vcf['POS'] <= new['end'])\n",
    "chr1_vcf = chr1_vcf[~(mask1 & mask2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_from_variant(variant):\n",
    "    # Split the variant string into its components\n",
    "    components = variant.strip().split(\"\\t\")\n",
    "\n",
    "    # Extract the frequency information from the INFO field\n",
    "    info = components[7]\n",
    "    frequency = None\n",
    "    for field in info.split(\";\"):\n",
    "        if field.startswith(\"AF=\"):\n",
    "            frequency = float(field[3:])\n",
    "            break\n",
    "    \n",
    "    # Return the frequency information\n",
    "    return frequency\n",
    "    \n",
    "# Open the VCF file and read the contents into a list of variants\n",
    "with open(\"DO52567.vcf\", \"r\") as f:\n",
    "    variants = f.readlines()\n",
    "\n",
    "# Extract the frequency information from each variant and store it in a separate list\n",
    "frequencies = []\n",
    "for variant in variants:\n",
    "    frequency = extract_frequency_from_variant(variant) # Custom function to extract frequency information from the variant\n",
    "    frequencies.append(frequency)\n",
    "\n",
    "# Sort the variants based on frequency\n",
    "sorted_variants = [variant for _, variant in sorted(zip(frequencies, variants))]\n",
    "\n",
    "# Write the sorted variants to a new VCF file\n",
    "with open(\"output.vcf\", \"w\") as f:\n",
    "    f.writelines(sorted_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame with ranges\n",
    "ranges_df = pd.DataFrame({'start': [100, 200, 300],\n",
    "                         'end': [150, 250, 350]})\n",
    "\n",
    "# Create the data frame with numbers\n",
    "numbers_df = pd.DataFrame({'number': [120, 230, 320, 390]})\n",
    "\n",
    "# Define a custom function to check if the number is in the range\n",
    "def is_in_range(num, start, end):\n",
    "    return (num >= start) & (num <= end)\n",
    "\n",
    "# Apply the custom function to each row of the numbers data frame using the ranges data frame\n",
    "result = parsed_df.apply(lambda row: new.apply(lambda x: is_in_range(row['POS'], x['start'], x['end']), axis=1).any(), axis=1)\n",
    "\n",
    "# The result is a series with a Boolean value for each number indicating if it is in any of the ranges\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyclone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4351b3529394af5914f7430d7b9357ce9dc4c1f619358be26ea52e4c761647a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
